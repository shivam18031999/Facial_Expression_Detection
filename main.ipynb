{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main File :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Packages and Modules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my model is stored in the form of json file so to get the model from json file i am using the module model_from_json\n",
    "\n",
    "from tensorflow.keras.models import model_from_json\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "\n",
    "# enabling tensor flow 1 \n",
    "\n",
    "config=tf.compat.v1.ConfigProto()\n",
    "session=tf.compat.v1.Session(config=config)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FacialExpressionModel Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this class is used to predict output(facial expression ) corrosponds to the face image\n",
    "# i will pass  two parametere to this class \n",
    "# first is a json file which contain my model archietecture \n",
    "# second is model_weight_file which contain the weights which i used to train my model\n",
    "\n",
    "class FacialExpressionModel(object):\n",
    "    # Emotion list \n",
    "    EMOTION_LIST=[\"Angry\",\"Disgust\",\"Fear\",\"Happy\",\"Neutral\",\"Sad\",\"Surprise\"]\n",
    "    \n",
    "    # this will take two parameters as input ,one is a json_file(model archietecture) and other one is weights\n",
    "    \n",
    "    def __init__ (self , model_json_file , model_weights_file):\n",
    "        \n",
    "        with open(model_json_file, \"r\") as json_file:\n",
    "             loaded_model_json=json_file.read()\n",
    "        \n",
    "        # making the model from json_file\n",
    "        self.loaded_model=model_from_json(loaded_model_json)\n",
    "        \n",
    "        # loading weights into the model\n",
    "        self.loaded_model.load_weights(model_weights_file)\n",
    "        \n",
    "        # making predict function\n",
    "        self.loaded_model._make_predict_function()\n",
    "    \n",
    "    # function used to predict emotion\n",
    "    def predict_emotion(self, img ):\n",
    "        \n",
    "        # self.preds is a list where each value of list indicate chance of that particular index class\n",
    "        self.preds=self.loaded_model.predict(img)\n",
    "\n",
    "        # using np.argmax to find the index of maximum value and then returning the emotion corrosponds to that emotion\n",
    "        return FacialExpressionModel.EMOTION_LIST[np.argmax(self.preds)] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capturing images and predicting output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Face not found\n",
      "Face not found\n",
      "Face not found\n",
      "Face not found\n"
     ]
    }
   ],
   "source": [
    "#using harcascade classifier to detect face in frame\n",
    "\n",
    "# creating object of CascadeClassifier\n",
    "face_classifier = cv2.CascadeClassifier(\"C:/Users/shivamGarg/AppData/Local/Programs/Python/Python37/Lib/site-packages/cv2/data/haarcascade_frontalface_default.xml\")\n",
    "\n",
    "# creating object of FacialExpressionModel , whose predict_emotion function is used to get the expression .\n",
    "# passing model_json (json file which contain model archietecture) and facial_weight_model ( weight of model )\n",
    "\n",
    "my_model = FacialExpressionModel(\"model_json\",\"model_weights.h5\")\n",
    "\n",
    "# using default webCam connected to pc\n",
    "video = cv2.VideoCapture(0) \n",
    "\n",
    "while True:\n",
    "    \n",
    "    # reading each frame one by one\n",
    "    _, img= video.read()\n",
    "    \n",
    "    # converting the image to grey scale \n",
    "    grey_img = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # detecting all faces in image\n",
    "    faces=face_classifier.detectMultiScale(grey_img,1.2,5)\n",
    "    \n",
    "    # if we found a face in image\n",
    "    if faces is not ():\n",
    "        \n",
    "        # Draw rectangles around each face and predicting output\n",
    "        for(x,y,w,h) in faces:\n",
    "            \n",
    "            # return a list of pixel value for face \n",
    "            fc = grey_img[y:y+h,x:x+w]\n",
    "            \n",
    "            # resizing the list into required image shape\n",
    "            face=cv2.resize(fc,(48,48))\n",
    "            \n",
    "            # use my_model to predict expression\n",
    "            pred = my_model.predict_emotion(face[np.newaxis,:,:,np.newaxis])\n",
    "            \n",
    "            # putting the predicted value text on image\n",
    "            cv2.putText(img,pred,(x,y),cv2.FONT_HERSHEY_COMPLEX,1,(255,255,0),1)\n",
    "            \n",
    "            # drawing rectangle around the face in image\n",
    "            cv2.rectangle(img,(x,y),(x+w,y+h),(255,0,0),2)\n",
    "        \n",
    "        # showing the image \n",
    "        cv2.imshow(\"img\",img)\n",
    "        \n",
    "    else :\n",
    "        print(\"Face not found\")\n",
    "        cv2.imshow(\"img\",img)\n",
    "   \n",
    "    # waitkey is used to stop the model\n",
    "    k=cv2.waitKey(1)\n",
    "    # 27 corrosponds to Esc \n",
    "    if k==27:\n",
    "        break\n",
    "\n",
    "# destroying all windows \n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# releasing the camera\n",
    "video.release()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
